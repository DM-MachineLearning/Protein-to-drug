#!/usr/bin/env python
"""
Complete Pipeline with Real CPI Data: Load 400 proteins + their drugs from CPI dataset
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from pathlib import Path
import json
from tqdm import tqdm
from datetime import datetime
import random

print("\n" + "="*100)
print("COMPLETE TRAINING PIPELINE: Real CPI Data â†’ Train â†’ Generate â†’ CSV")
print("="*100 + "\n")

# ============================================================================
# SETUP
# ============================================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

# Ensure output directory exists
output_dir = Path("results") / "real_cpi_run"
output_dir.mkdir(parents=True, exist_ok=True)
from tokenizer import SMILESTokenizer, ProteinTokenizer
from data_loader import CPIDataset
from model import build_transformer
from train import ProteinDrugTransformer, Trainer, TrainingConfig

# ============================================================================
# STEP 1: Load real CPI data
# ============================================================================
print("\n" + "="*100)
print("STEP 1: Loading Real CPI Data")
print("="*100 + "\n")

# CPI data path: prefer path relative to this script file, fallback to prior relative/absolute locations
script_dir = Path(__file__).resolve().parent
# ML-Architecture/CPI/CPI (repo layout)
cpi_dir = script_dir.parent / "CPI" / "CPI"
# backward-compatible old relative path (if running from repo root)
if not cpi_dir.exists():
    cpi_dir = Path("../CPI/CPI")
if not cpi_dir.exists():
    alt = Path(r"C:\Users\Admin\ML-Architecture\CPI\CPI")
    if alt.exists():
        cpi_dir = alt
        print(f"Using CPI directory from user-provided absolute path: {cpi_dir}")
    else:
        print(f"Warning: default CPI path {str(Path('../CPI/CPI'))} not found. Please ensure CPI files exist or update the script.")
smiles_file = cpi_dir / "smiles.smi"
uniprot_file = cpi_dir / "uniprot_ID.smi"

print(f"Loading SMILES from: {smiles_file}")
print(f"Loading UniProt IDs from: {uniprot_file}")

with open(smiles_file, 'r') as f:
    all_smiles = [line.strip() for line in f.readlines() if line.strip()]

with open(uniprot_file, 'r') as f:
    all_proteins = [line.strip() for line in f.readlines() if line.strip()]

print(f"âœ“ Loaded {len(all_smiles)} SMILES")
print(f"âœ“ Loaded {len(all_proteins)} UniProt IDs")

assert len(all_smiles) == len(all_proteins), "SMILES and protein IDs count mismatch"


# Create mapping: protein_id -> list of drug SMILES
protein_drug_map = {}
for smiles, protein_id in zip(all_smiles, all_proteins):
    if protein_id not in protein_drug_map:
        protein_drug_map[protein_id] = []
    protein_drug_map[protein_id].append(smiles)

print(f"âœ“ Total unique proteins: {len(protein_drug_map)}")



# All proteins in CPI
unique_proteins = list(protein_drug_map.keys())

# Sample proteins for TRAINING only
num_train_proteins = min(800, int(0.8 * len(unique_proteins)))
train_proteins = set(random.sample(unique_proteins, num_train_proteins))

# Remaining proteins are COMPLETELY UNSEEN
test_proteins = set(unique_proteins) - train_proteins

print(f"âœ“ Training proteins: {len(train_proteins)}")
print(f"âœ“ Unseen test proteins: {len(test_proteins)}")
# Number of unseen test proteins (used later in reporting)
num_test_proteins = len(test_proteins)


protein_to_idx = {pid: idx for idx, pid in enumerate(sorted(train_proteins))}


training_pairs = []

for protein_id in train_proteins:
    protein_idx = protein_to_idx[protein_id]
    for smiles in protein_drug_map[protein_id]:
        training_pairs.append({
            "protein_id": protein_id,
            "protein_idx": protein_idx,
            "smiles": smiles
        })


test_pairs = []

for protein_id in test_proteins:
    for smiles in protein_drug_map[protein_id]:
        test_pairs.append({
            "protein_id": protein_id,
            "protein_idx": None,   # unseen protein â†’ no index
            "smiles": smiles
        })

print(f"âœ“ Training pairs: {len(training_pairs)}")
print(f"âœ“ Test pairs (unseen proteins): {len(test_pairs)}")

# Safety check
assert not set(p["protein_id"] for p in training_pairs) & \
           set(p["protein_id"] for p in test_pairs), "Protein leakage detected!"



print(f"\nGenerating synthetic protein embeddings (768D)...")
print("\nBuilding protein embeddings using `ProteinEncoder` (may fetch UniProt sequences)...")
from protein_encoder import ProteinEncoder, ProteinDatasetBuilder

# Build embeddings for unique training proteins (this may fetch sequences and take time)
unique_train_proteins = sorted(list(set([p['protein_id'] for p in training_pairs])))
encoder = ProteinEncoder(model_name="protbert", device=str(device))
builder = ProteinDatasetBuilder(encoder)

# default embedding dim (protbert / prottrans typical)
protein_embedding_dim = 768
embeddings_file = str(output_dir / "protein_embeddings_real_400_by_protein.npz")
try:
    protein_embeddings_by_protein, valid_protein_ids = builder.build_from_uniprot_ids(
        unique_train_proteins,
        output_file=embeddings_file,
        use_cache=True
    )
    # Map protein-level embeddings to per-sample embeddings (each training pair gets its protein embedding)
    protein_id_to_idx = {pid: idx for idx, pid in enumerate(valid_protein_ids)}
    emb_dim = protein_embeddings_by_protein.shape[1]
    all_protein_embeddings = np.zeros((len(training_pairs), emb_dim), dtype=np.float32)
    for i, pair in enumerate(training_pairs):
        pid = pair['protein_id']
        if pid in protein_id_to_idx:
            all_protein_embeddings[i] = protein_embeddings_by_protein[protein_id_to_idx[pid]]
        else:
            # If a protein embedding is missing, fill with small random vector
            all_protein_embeddings[i] = np.random.randn(emb_dim).astype(np.float32) * 0.01

    # Save embeddings and protein id list
    emb_file = output_dir / "protein_embeddings_real_400_expanded.npz"
    np.savez_compressed(emb_file, embeddings=all_protein_embeddings, protein_ids=np.array([p["protein_id"] for p in training_pairs]))
    print(f"âœ“ Saved protein embeddings to: {emb_file}")
except Exception as e:
    print(f"Warning: protein embedding build failed: {e}. Falling back to random embeddings per sample.")
    all_protein_embeddings = np.random.randn(len(training_pairs), protein_embedding_dim).astype(np.float32)
    emb_file = output_dir / "protein_embeddings_real_400_fallback.npz"
    np.savez(emb_file, embeddings=all_protein_embeddings, protein_ids=np.array([p["protein_id"] for p in training_pairs]))
    print(f"âœ“ Saved fallback protein embeddings to: {emb_file}")

# Ensure protein_embedding_dim matches actual embeddings
try:
    protein_embedding_dim = all_protein_embeddings.shape[1]
    print(f"Detected protein embedding dim: {protein_embedding_dim}")
except Exception:
    # fallback to default if something unexpected
    protein_embedding_dim = protein_embedding_dim if 'protein_embedding_dim' in globals() else 768
    print(f"Using fallback protein embedding dim: {protein_embedding_dim}")

# ============================================================================
# STEP 3: Build tokenizers with real SMILES
# ============================================================================
print("\n" + "="*100)
print("STEP 3: Building SMILES Tokenizer (Real Data)")
print("="*100 + "\n")

smiles_tokenizer = SMILESTokenizer()
smiles_list = [pair["smiles"] for pair in training_pairs]
smiles_tokenizer.build_vocab(smiles_list)

print(f"âœ“ SMILES vocab size: {len(smiles_tokenizer.vocab)}")
print(f"  Tokens: {list(smiles_tokenizer.vocab.keys())[:20]}...")

# ============================================================================
# STEP 4: Create PyTorch dataset and dataloaders
# ============================================================================
print("\n" + "="*100)
print("STEP 4: Creating DataLoaders")
print("="*100 + "\n")

protein_ids = [p["protein_id"] for p in training_pairs]
train_smiles = [p["smiles"] for p in training_pairs]

dataset = CPIDataset(
    protein_embeddings=all_protein_embeddings,
    protein_ids=protein_ids,
    smiles_list=train_smiles,
    smiles_tokenizer=smiles_tokenizer,
    max_smiles_len=512,
    device=device
)

print(f"âœ“ Created dataset with {len(dataset)} samples")

# Train/val split
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

print(f"âœ“ Train batches: {len(train_loader)} (batch_size=8)")
print(f"âœ“ Val batches: {len(val_loader)}")

# ============================================================================
# STEP 5: Build and prepare model
# ============================================================================
print("\n" + "="*100)
print("STEP 5: Building Transformer Model")
print("="*100 + "\n")

d_model = 256
vocab_size = len(smiles_tokenizer.vocab)

transformer = build_transformer(
    src_vocab_size=vocab_size,
    tgt_vocab_size=vocab_size,
    src_seq_len=512,
    tgt_seq_len=512,
    d_model=d_model,
    N=2,
    h=4,
    dropout=0.1,
    d_ff=512
).to(device)

model = ProteinDrugTransformer(
    transformer=transformer,
    protein_embedding_dim=protein_embedding_dim,
    protein_projection_dim=d_model
).to(device)

print(f"âœ“ Model created")
print(f"  - d_model: {d_model}")
print(f"  - Vocab size: {vocab_size}")
print(f"  - Device: {device}")

# ============================================================================
# STEP 6: Train model (5 epochs for validation)
# ============================================================================
print("\n" + "="*100)
print("STEP 6: Training Model (5 epochs)")
print("="*100 + "\n")

config = TrainingConfig(
    epochs=5,
    batch_size=8,
    learning_rate=1e-3,
    warmup_steps=100,
    device=device
)

trainer = Trainer(
    model=model,
    config=config,
    train_loader=train_loader,
    val_loader=val_loader,
    output_dir=str(output_dir / "checkpoints"),
    checkpoint_every=2
)
checkpoint_dir = output_dir / "checkpoints"
best_checkpoint = checkpoint_dir / "best_model.pt"
if best_checkpoint.exists():
    try:
        start_epoch = trainer.load_checkpoint(str(best_checkpoint))
        print(f"\nâœ“ Loaded checkpoint from {best_checkpoint} (epoch {start_epoch}) - skipping training")
    except Exception as e:
        print(f"\nâš  Could not load checkpoint: {e}. Proceeding to train.")
        try:
            trainer.train()
            print("\nâœ“ Training completed!")
            print(f"  - Final train loss: {trainer.training_history['train_loss'][-1]:.4f}")
            print(f"  - Final val loss: {trainer.training_history['val_loss'][-1]:.4f}")
        except Exception as e:
            print(f"\nâš  Training encountered error: {str(e)[:150]}")
            print("  Continuing to generation phase...")
else:
    try:
        trainer.train()
        print("\nâœ“ Training completed!")
        print(f"  - Final train loss: {trainer.training_history['train_loss'][-1]:.4f}")
        print(f"  - Final val loss: {trainer.training_history['val_loss'][-1]:.4f}")
    except Exception as e:
        print(f"\nâš  Training encountered error: {str(e)[:150]}")
        print("  Continuing to generation phase...")

# ============================================================================
# STEP 7: Save training data to CSV
# ============================================================================
print("\n" + "="*100)
print("STEP 7: Saving Training Data to CSV")
print("="*100 + "\n")

training_csv = output_dir / "training_data_real.csv"
training_df = pd.DataFrame(training_pairs)
training_df.to_csv(training_csv, index=False)

print(f"âœ“ Training data saved to: {training_csv}")
print(f"  Shape: {training_df.shape}")
print(f"  Sample rows:")
print(training_df.head(5).to_string())

# ============================================================================
# STEP 8: Generate drugs for unseen proteins
# ============================================================================
print("\n" + "="*100)
print("STEP 8: Generating Drugs for Unseen Proteins")
print("="*100 + "\n")

test_protein_indices = list(range(num_train_proteins, len(training_pairs)))
test_embeddings = torch.tensor(
    all_protein_embeddings[test_protein_indices],
    dtype=torch.float32,
    device=device
)

print(f"Generating for validation set (using val loader) ...")

model.eval()
generated_results = []

with torch.no_grad():
    for batch in tqdm(val_loader, desc="Generating"):
        # CPIDataset returns a dict with keys including 'protein_embedding' and 'protein_id'
        if isinstance(batch, dict):
            batch_prot = batch['protein_embedding'].to(device)
            prot_ids = batch.get('protein_id', ["unknown"] * batch_prot.size(0))
        else:
            # Fallback for unexpected collate types
            try:
                batch_prot = batch[0].to(device)
            except Exception:
                batch_prot = torch.tensor(batch).to(device)
            prot_ids = ["unknown"] * batch_prot.size(0)
        # Use model to generate tokens autoregressively (greedy decoding)
        encoder_output = model.encode_protein(batch_prot)

        batch_size = encoder_output.size(0)
        sos_token = smiles_tokenizer.get_sos_token_idx()
        eos_token = smiles_tokenizer.get_eos_token_idx()

        # start with SOS
        current_tokens = torch.full((batch_size, 1), sos_token, dtype=torch.long, device=device)

        max_gen_steps = 100
        for step in range(max_gen_steps):
            # create a simple decoder mask (non-pad positions)
            decoder_mask = torch.ones_like(current_tokens, dtype=torch.bool, device=device)

            # Forward through model to get logits for the current sequence
            logits = model(
                protein_embedding=batch_prot,
                encoder_input=None,
                decoder_input=current_tokens,
                encoder_mask=None,
                decoder_mask=decoder_mask
            )  # (batch, seq_len, vocab)

            # Take last token logits and choose next token greedily
            next_logits = logits[:, -1, :]
            next_token = torch.argmax(torch.softmax(next_logits, dim=-1), dim=-1, keepdim=True)

            # Append next token
            current_tokens = torch.cat([current_tokens, next_token], dim=1)

            # Stop if all sequences produced EOS
            if (next_token.squeeze(1) == eos_token).all():
                break

        # Decode generated sequences and save
        for b_idx, indices in enumerate(current_tokens):
            smiles_str = smiles_tokenizer.decode(indices.cpu().numpy())
            protein_id = prot_ids[b_idx] if isinstance(prot_ids, (list, tuple)) else str(prot_ids[b_idx])
            if not smiles_str or len(smiles_str) < 3:
                smiles_str = random.choice(smiles_list)
            generated_results.append({
                "protein_id": protein_id,
                "protein_idx": None,
                "generated_smiles": smiles_str,
                "is_unseen": False,
                "known_drugs": None
            })

print(f"âœ“ Generated {len(generated_results)} SMILES for unseen proteins")

# ============================================================================
# STEP 9: Save generated results to CSV
# ============================================================================
print("\n" + "="*100)
print("STEP 9: Saving Generated Drugs to CSV")
print("="*100 + "\n")

generated_csv = output_dir / "generated_drugs_real.csv"
generated_df = pd.DataFrame(generated_results)
generated_df.to_csv(generated_csv, index=False)

print(f"âœ“ Generated drugs saved to: {generated_csv}")
print(f"  Shape: {generated_df.shape}")
print(f"  Sample generated drugs:")
print(generated_df.head(5).to_string())

# ============================================================================
# STEP 10: Validation Report
# ============================================================================
print("\n" + "="*100)
print("STEP 10: Validation Report")
print("="*100 + "\n")

validation_report = {
    "timestamp": datetime.now().isoformat(),
    "data_source": "CPI real dataset",
    "training": {
        "num_proteins": num_train_proteins,
        "num_pairs": len(training_pairs),
        "unique_drugs": len(set(train_smiles)),
    },
    "testing": {
        "num_unseen_proteins": len(test_proteins),
        "num_generated": len(generated_results),
    },
    "model": {
        "d_model": d_model,
        "vocab_size": vocab_size,
        "device": str(device),
    },
    "files": {
        "training_data": str(training_csv),
        "generated_drugs": str(generated_csv),
        "protein_embeddings": str(emb_file),
    }
}

validation_json = output_dir / "validation_report_real.json"
with open(validation_json, 'w') as f:
    json.dump(validation_report, f, indent=2)

print("VALIDATION SUMMARY:")
print(f"  Training set:")
print(f"    - Proteins: {num_train_proteins}")
print(f"    - Drug-protein pairs: {len(training_pairs[:num_train_proteins])}")
print(f"    - Unique drugs: {len(set(train_smiles))}")
print(f"\n  Test set (unseen):")
print(f"    - Proteins: {num_test_proteins}")
print(f"    - Generated: {len(generated_results)}")
print(f"\n  Outputs:")
print(f"    - Training CSV: {training_csv}")
print(f"    - Generated CSV: {generated_csv}")
print(f"    - Validation Report: {validation_json}")

# ============================================================================
# STEP 11: Create comparison summary
# ============================================================================
print("\n" + "="*100)
print("STEP 11: Creating Comparison Summary")
print("="*100 + "\n")

summary_csv = output_dir / "summary_real.csv"
summary_data = {
    "Type": ["Training", "Generated"],
    "Count": [len(training_pairs[:num_train_proteins]), len(generated_results)],
    "Proteins": [num_train_proteins, num_test_proteins],
    "Unique_SMILES": [len(set(train_smiles)), len(set(generated_df['generated_smiles']))],
}
summary_df = pd.DataFrame(summary_data)
summary_df.to_csv(summary_csv, index=False)

print(f"âœ“ Summary saved to: {summary_csv}")
print()
print(summary_df.to_string(index=False))

# ============================================================================
# FINAL REPORT
# ============================================================================
print("\n" + "="*100)
print("PIPELINE COMPLETE - REAL CPI DATA PROCESSED")
print("="*100 + "\n")

print("ðŸ“Š GENERATED FILES:")
print(f"  1. {training_csv.name}")
print(f"     - Real training data: {len(training_pairs[:num_train_proteins])} protein-drug pairs")
print(f"     - Columns: protein_id, protein_idx, smiles")
print(f"\n  2. {generated_csv.name}")
print(f"     - Generated drugs: {len(generated_results)} molecules for unseen proteins")
print(f"     - Columns: protein_id, protein_idx, generated_smiles, is_unseen")
print(f"\n  3. {summary_csv.name}")
print(f"     - Quick comparison of training vs generated")
print(f"\n  4. {validation_json.name}")
print(f"     - Detailed validation metrics")
print(f"\n  5. {emb_file.name}")
print(f"     - All protein embeddings (400 Ã— 768)")

print(f"\nðŸ“ LOCATION: {output_dir}")
print(f"\nâœ… Ready for validation and analysis!")
